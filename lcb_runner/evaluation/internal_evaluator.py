"""
Internal Evaluation Module for RL Self-Repair

This module evaluates code against synthetic tests generated by the test generator.
It provides the reward signal for the RL loop without using ground-truth tests.
"""

import json
import logging
import subprocess
import tempfile
from typing import List, Dict, Any, Tuple
import signal
import os

from lcb_runner.evaluation.synthetic_test_generator import TestCase


class InternalEvaluator:
    """
    Evaluates code against synthetic test cases.
    Provides pass/fail results and metadata without using dataset tests.
    """
    
    def __init__(self, timeout: float = 6.0):
        self.timeout = timeout
    
    def evaluate_on_synthetic_tests(
        self,
        code: str,
        tests: List[TestCase],
        code_type: str = "stdin"
    ) -> Tuple[List[bool], Dict[str, Any]]:
        """
        Evaluate code against synthetic test cases.
        
        Args:
            code: Python code to evaluate
            tests: List of TestCase objects
            code_type: "stdin" or "call_based"
            
        Returns:
            Tuple of (pass_results, metadata)
            - pass_results: List of booleans indicating pass/fail for each test
            - metadata: Dict with test execution details
        """
        if not tests:
            return [], {"error": "No tests provided"}
        
        results = []
        metadata = {
            "test_results": [],
            "errors": [],
            "timeouts": 0,
            "execution_times": []
        }
        
        for idx, test_case in enumerate(tests):
            try:
                if code_type == "call_based":
                    passed, test_metadata = self._evaluate_call_based(code, test_case)
                else:  # stdin
                    passed, test_metadata = self._evaluate_stdin(code, test_case)
                
                results.append(passed)
                metadata["test_results"].append({
                    "index": idx,
                    "category": test_case.category,
                    "confidence": test_case.confidence,
                    "passed": passed,
                    "metadata": test_metadata
                })
                
                # Debug logging - use original_index if available to show which test from generation phase
                test_label = test_case.original_index if hasattr(test_case, 'original_index') and test_case.original_index is not None else idx
                if not passed:
                    print(f"Test {test_label} FAILED - Input: {repr(str(test_case.input_val)[:50])}, Expected: {repr(test_case.expected_output)}, Actual: {repr(test_metadata.get('actual', 'N/A'))}")
                    if 'error' in test_metadata:
                        print(f"  Error: {test_metadata['error']}")
                else:
                    print(f"Test {test_label} PASSED - Category: {test_case.category}")
                
                if test_metadata.get("timeout"):
                    metadata["timeouts"] += 1
                if "execution_time" in test_metadata:
                    metadata["execution_times"].append(test_metadata["execution_time"])
                if "error" in test_metadata:
                    metadata["errors"].append({
                        "test": idx,
                        "error": test_metadata["error"]
                    })
            
            except Exception as e:
                logging.warning(f"Test {idx} evaluation failed: {e}")
                results.append(False)
                metadata["errors"].append({
                    "test": idx,
                    "error": repr(e)
                })
        
        return results, metadata
    
    def _evaluate_stdin(self, code: str, test: TestCase) -> Tuple[bool, Dict[str, Any]]:
        """Evaluate stdin-based code against a test case."""
        import time
        
        try:
            # Check if code is empty or invalid
            if not code or code.strip() == "":
                return False, {
                    "error": "Code is empty or invalid",
                    "error_code": -5
                }
            
            # Create a temporary Python file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_file = f.name
            
            try:
                # Prepare input - ensure it's properly formatted
                if isinstance(test.input_val, str):
                    input_data = test.input_val
                elif isinstance(test.input_val, dict):
                    # If it's a dict, try to extract the actual input
                    if "input" in test.input_val:
                        input_data = test.input_val["input"]
                    else:
                        # Fall back to converting the whole dict to string representation
                        input_data = str(test.input_val)
                else:
                    # For other types (list, etc.), convert to string
                    input_data = str(test.input_val)
                
                # Ensure input ends with newline if it doesn't already
                if input_data and not input_data.endswith('\n'):
                    input_data += '\n'
                
                # Run with timeout
                start_time = time.time()
                process = subprocess.Popen(
                    ['python', temp_file],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                try:
                    stdout, stderr = process.communicate(
                        input=input_data, 
                        timeout=self.timeout
                    )
                    execution_time = time.time() - start_time
                    
                    if process.returncode != 0:
                        # Capture stderr for debugging
                        error_msg = stderr.strip() if stderr else "Unknown error"
                        return False, {
                            "error": error_msg[:200],  # Truncate long errors
                            "error_code": -4,
                            "execution_time": execution_time,
                            "input_used": input_data,
                            "stderr": stderr
                        }
                    
                    # Normalize stdout: strip trailing spaces/newlines but preserve structure
                    actual_output = stdout.rstrip('\n\r ').strip()
                    expected_output = str(test.expected_output).strip()
                    
                    # Compare outputs with normalization
                    passed = self._compare_outputs(actual_output, expected_output)
                    
                    return passed, {
                        "actual": actual_output,
                        "expected": expected_output,
                        "execution_time": execution_time,
                        "confidence": test.confidence,
                        "input_used": input_data
                    }
                
                except subprocess.TimeoutExpired:
                    process.kill()
                    return False, {
                        "timeout": True,
                        "error_code": -3,
                        "error": "Time limit exceeded",
                        "input_used": input_data
                    }
            
            finally:
                os.unlink(temp_file)
        
        except Exception as e:
            return False, {
                "error": repr(e),
                "error_code": -4
            }
    
    def _evaluate_call_based(self, code: str, test: TestCase) -> Tuple[bool, Dict[str, Any]]:
        """Evaluate call-based code against a test case."""
        import time
        import re
        
        try:
            # Check if code is empty or invalid
            if not code or code.strip() == "":
                return False, {
                    "error": "Code is empty or invalid",
                    "error_code": -5
                }
            
            # Extract function/method name and check if it's inside a class
            func_name = None
            class_name = None
            
            lines = code.split('\n')
            for i, line in enumerate(lines):
                # Check for class definition
                class_match = re.match(r'\s*class\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)
                if class_match:
                    class_name = class_match.group(1)
                
                # Check for function/method definition
                func_match = re.match(r'\s*def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                if func_match:
                    func_name = func_match.group(1)
                    # If we found a method inside a class, we're done
                    if class_name:
                        break
            
            if not func_name:
                return False, {
                    "error": "Could not find function definition in code",
                    "error_code": -5
                }
            
            # Create test wrapper
            # Call-based inputs are always parsed as JSON strings (see grade_call_based in testing_util.py)
            # We need to handle the input correctly based on whether it's already a list/dict or a JSON string
            if isinstance(test.input_val, dict) or isinstance(test.input_val, list):
                # Already a structured object, convert to JSON and then parse
                input_json_str = json.dumps(test.input_val)
                
                # If function is inside a class, instantiate it first
                if class_name:
                    test_wrapper = f"""
{code}

# Test execution
import json

input_data = json.loads('{input_json_str}')
obj = {class_name}()
result = obj.{func_name}(*input_data) if isinstance(input_data, list) else obj.{func_name}(input_data)
print(json.dumps(result))
"""
                else:
                    test_wrapper = f"""
{code}

# Test execution
import json

input_data = json.loads('{input_json_str}')
result = {func_name}(*input_data) if isinstance(input_data, list) else {func_name}(input_data)
print(json.dumps(result))
"""
            elif isinstance(test.input_val, str):
                # Could be a JSON string or raw string
                try:
                    # Try to parse as JSON first
                    parsed = json.loads(test.input_val)
                    if isinstance(parsed, list):
                        if class_name:
                            test_wrapper = f"""
{code}

# Test execution
import json

input_data = json.loads({repr(test.input_val)})
obj = {class_name}()
result = obj.{func_name}(*input_data)
print(json.dumps(result))
"""
                        else:
                            test_wrapper = f"""
{code}

# Test execution
import json

input_data = json.loads({repr(test.input_val)})
result = {func_name}(*input_data)
print(json.dumps(result))
"""
                    else:
                        if class_name:
                            test_wrapper = f"""
{code}

# Test execution
import json
input_data = json.loads({repr(test.input_val)})
obj = {class_name}()
result = obj.{func_name}(input_data)
print(json.dumps(result))
"""
                        else:
                            test_wrapper = f"""
{code}

# Test execution
import json
input_data = json.loads({repr(test.input_val)})
result = {func_name}(input_data)
print(json.dumps(result))
"""
                except json.JSONDecodeError:
                    # Not valid JSON, reject this test input as malformed
                    return False, {
                        "error": "Input is not valid JSON for call_based function",
                        "error_code": -4
                    }
            else:
                # Other types (int, float, etc.) - treat as single argument
                if class_name:
                    test_wrapper = f"""
{code}

# Test execution
obj = {class_name}()
result = obj.{func_name}({repr(test.input_val)})
print(json.dumps(result))
"""
                else:
                    test_wrapper = f"""
{code}

# Test execution
result = {func_name}({repr(test.input_val)})
print(json.dumps(result))
"""
            
            # Create temporary file and execute
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(test_wrapper)
                temp_file = f.name
            
            try:
                start_time = time.time()
                process = subprocess.Popen(
                    ['python', temp_file],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                try:
                    stdout, stderr = process.communicate(timeout=self.timeout)
                    execution_time = time.time() - start_time
                    
                    if process.returncode != 0:
                        # Capture stderr for debugging
                        error_msg = stderr.strip() if stderr else "Unknown error"
                        return False, {
                            "error": error_msg[:200],  # Truncate long errors
                            "error_code": -4,
                            "execution_time": execution_time
                        }
                    
                    # Parse and normalize output
                    try:
                        # Try JSON parsing first
                        actual_output = json.loads(stdout.strip())
                    except json.JSONDecodeError:
                        # Fallback to string with normalization
                        actual_output = stdout.rstrip('\n\r ').strip()
                    
                    passed = self._compare_outputs(actual_output, test.expected_output)
                    
                    return passed, {
                        "actual": actual_output,
                        "expected": test.expected_output,
                        "execution_time": execution_time,
                        "confidence": test.confidence
                    }
                
                except subprocess.TimeoutExpired:
                    process.kill()
                    return False, {
                        "timeout": True,
                        "error_code": -3,
                        "error": "Time limit exceeded"
                    }
            
            finally:
                os.unlink(temp_file)
        
        except Exception as e:
            return False, {
                "error": repr(e),
                "error_code": -4
            }
    
    def _compare_outputs(self, actual: Any, expected: Any, float_tol: float = 1e-6) -> bool:
        """
        Compare actual and expected outputs with normalization.
        Handles JSON, floats with tolerance, and normalized string comparisons.
        """
        # Try direct comparison first
        if actual == expected:
            return True
        
        # Convert to strings and normalize
        actual_str = str(actual).strip().rstrip('\n\r ')
        expected_str = str(expected).strip().rstrip('\n\r ')
        
        if actual_str == expected_str:
            return True
        
        # Handle boolean comparisons (Python True/False vs JSON true/false vs string "true"/"false")
        if actual_str.lower() in ('true', 'false') and expected_str.lower() in ('true', 'false'):
            return actual_str.lower() == expected_str.lower()
        
        # Try JSON comparison
        try:
            actual_json = json.loads(actual_str)
            expected_json = json.loads(expected_str)
            return actual_json == expected_json
        except (json.JSONDecodeError, ValueError):
            pass
        
        # Try float comparison with tolerance
        try:
            actual_float = float(actual_str)
            expected_float = float(expected_str)
            return abs(actual_float - expected_float) <= float_tol
        except (ValueError, TypeError):
            pass
        
        # For multi-line outputs, compare line by line
        actual_lines = actual_str.split('\n')
        expected_lines = expected_str.split('\n')
        
        if len(actual_lines) == len(expected_lines):
            all_match = True
            for a_line, e_line in zip(actual_lines, expected_lines):
                a_line = a_line.strip()
                e_line = e_line.strip()
                
                # Try numeric comparison per line
                try:
                    if abs(float(a_line) - float(e_line)) > float_tol:
                        all_match = False
                        break
                except (ValueError, TypeError):
                    # String comparison with normalization
                    if self._normalize_string(a_line) != self._normalize_string(e_line):
                        all_match = False
                        break
            
            if all_match:
                return True
        
        # Fallback to normalized string comparison
        return self._normalize_string(actual_str) == self._normalize_string(expected_str)
    
    def _normalize_string(self, s: str) -> str:
        """Normalize string for comparison."""
        # Remove trailing newlines and spaces
        s = s.rstrip('\n\r ')
        # Collapse multiple spaces/newlines to single space
        import re
        s = re.sub(r'\s+', ' ', s)
        # Normalize simple list formats (remove spaces after commas)
        s = s.replace(', ', ',')
        return s


def compute_weighted_reward(
    test_results: List[bool],
    test_confidences: List[float],
    old_results: List[bool] = None
) -> float:
    """
    Compute reward from test results with optional confidence weighting.
    
    Args:
        test_results: List of pass/fail results
        test_confidences: List of confidence scores for each test
        old_results: Previous test results for computing delta
        
    Returns:
        Reward value (typically -0.5 to 1.0)
    """
    if not test_results:
        return 0.0
    
    # If all tests pass
    if all(test_results):
        return 1.0
    
    # Compute weighted pass rate
    weighted_passes = sum(p * c for p, c in zip(test_results, test_confidences))
    weighted_total = sum(test_confidences)
    
    if weighted_total == 0:
        return 0.0
    
    current_score = weighted_passes / weighted_total
    
    # If we have old results, compute improvement
    if old_results is not None and len(old_results) == len(test_results):
        old_weighted_passes = sum(p * c for p, c in zip(old_results, test_confidences))
        old_score = old_weighted_passes / weighted_total
        
        improvement = current_score - old_score
        
        if improvement > 0:
            return improvement  # Reward for improvement
        else:
            return -0.5  # Penalty for no improvement or regression
    else:
        # No baseline to compare, just return score
        return current_score - 0.5  # Center around 0

