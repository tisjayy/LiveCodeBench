"""
Internal Evaluation Module for RL Self-Repair

This module evaluates code against synthetic tests generated by the test generator.
It provides the reward signal for the RL loop without using ground-truth tests.
"""

import json
import logging
import subprocess
import tempfile
from typing import List, Dict, Any, Tuple
import signal
import os

from lcb_runner.evaluation.synthetic_test_generator import TestCase


class InternalEvaluator:
    """
    Evaluates code against synthetic test cases.
    Provides pass/fail results and metadata without using dataset tests.
    """
    
    def __init__(self, timeout: float = 6.0):
        self.timeout = timeout
    
    def evaluate_on_synthetic_tests(
        self,
        code: str,
        tests: List[TestCase],
        code_type: str = "stdin"
    ) -> Tuple[List[bool], Dict[str, Any]]:
        """
        Evaluate code against synthetic test cases.
        
        Args:
            code: Python code to evaluate
            tests: List of TestCase objects
            code_type: "stdin" or "call_based"
            
        Returns:
            Tuple of (pass_results, metadata)
            - pass_results: List of booleans indicating pass/fail for each test
            - metadata: Dict with test execution details
        """
        if not tests:
            return [], {"error": "No tests provided"}
        
        results = []
        metadata = {
            "test_results": [],
            "errors": [],
            "timeouts": 0,
            "execution_times": []
        }
        
        for idx, test_case in enumerate(tests):
            try:
                if code_type == "call_based":
                    passed, test_metadata = self._evaluate_call_based(code, test_case)
                else:  # stdin
                    passed, test_metadata = self._evaluate_stdin(code, test_case)
                
                results.append(passed)
                metadata["test_results"].append({
                    "index": idx,
                    "category": test_case.category,
                    "confidence": test_case.confidence,
                    "passed": passed,
                    "metadata": test_metadata
                })
                
                if test_metadata.get("timeout"):
                    metadata["timeouts"] += 1
                if "execution_time" in test_metadata:
                    metadata["execution_times"].append(test_metadata["execution_time"])
                if "error" in test_metadata:
                    metadata["errors"].append({
                        "test": idx,
                        "error": test_metadata["error"]
                    })
            
            except Exception as e:
                logging.warning(f"Test {idx} evaluation failed: {e}")
                results.append(False)
                metadata["errors"].append({
                    "test": idx,
                    "error": repr(e)
                })
        
        return results, metadata
    
    def _evaluate_stdin(self, code: str, test: TestCase) -> Tuple[bool, Dict[str, Any]]:
        """Evaluate stdin-based code against a test case."""
        import time
        
        try:
            # Create a temporary Python file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_file = f.name
            
            try:
                # Prepare input - if it's already a string, use it directly
                # Otherwise, try to convert to string
                if isinstance(test.input_val, str):
                    input_data = test.input_val
                elif isinstance(test.input_val, dict):
                    # If it's a dict, try to extract the actual input
                    # This handles cases where the LLM might return structured data
                    if "input" in test.input_val:
                        input_data = test.input_val["input"]
                    else:
                        # Fall back to converting the whole dict to JSON
                        input_data = json.dumps(test.input_val)
                else:
                    # For other types (list, etc.), convert to JSON string
                    input_data = json.dumps(test.input_val)
                
                
                # Run with timeout
                start_time = time.time()
                process = subprocess.Popen(
                    ['python', temp_file],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                try:
                    stdout, stderr = process.communicate(
                        input=input_data, 
                        timeout=self.timeout
                    )
                    execution_time = time.time() - start_time
                    
                    if process.returncode != 0:
                        # Capture stderr for debugging
                        error_msg = stderr.strip() if stderr else "Unknown error"
                        return False, {
                            "error": error_msg[:200],  # Truncate long errors
                            "error_code": -4,
                            "execution_time": execution_time
                        }
                    
                    # Normalize stdout: strip trailing spaces/newlines
                    actual_output = stdout.rstrip('\n\r ').strip()
                    expected_output = str(test.expected_output).strip()
                    
                    # Compare outputs with normalization
                    passed = self._compare_outputs(actual_output, expected_output)
                    
                    return passed, {
                        "actual": actual_output,
                        "expected": expected_output,
                        "execution_time": execution_time,
                        "confidence": test.confidence
                    }
                
                except subprocess.TimeoutExpired:
                    process.kill()
                    return False, {
                        "timeout": True,
                        "error_code": -3,
                        "error": "Time limit exceeded"
                    }
            
            finally:
                os.unlink(temp_file)
        
        except Exception as e:
            return False, {
                "error": repr(e),
                "error_code": -4
            }
    
    def _evaluate_call_based(self, code: str, test: TestCase) -> Tuple[bool, Dict[str, Any]]:
        """Evaluate call-based code against a test case."""
        import time
        
        try:
            # Extract function name from test (would need metadata)
            # For now, assume "solution" or try to extract
            
            # Create test wrapper
            test_wrapper = f"""
{code}

# Test execution
import json
input_data = json.loads('{json.dumps(test.input_val)}')
result = solution(input_data) if 'solution' in dir() else None
print(json.dumps(result))
"""
            
            # Create temporary file and execute
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(test_wrapper)
                temp_file = f.name
            
            try:
                start_time = time.time()
                process = subprocess.Popen(
                    ['python', temp_file],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                try:
                    stdout, stderr = process.communicate(timeout=self.timeout)
                    execution_time = time.time() - start_time
                    
                    if process.returncode != 0:
                        # Capture stderr for debugging
                        error_msg = stderr.strip() if stderr else "Unknown error"
                        return False, {
                            "error": error_msg[:200],  # Truncate long errors
                            "error_code": -4,
                            "execution_time": execution_time
                        }
                    
                    # Parse and normalize output
                    try:
                        # Try JSON parsing first
                        actual_output = json.loads(stdout.strip())
                    except json.JSONDecodeError:
                        # Fallback to string with normalization
                        actual_output = stdout.rstrip('\n\r ').strip()
                    
                    passed = self._compare_outputs(actual_output, test.expected_output)
                    
                    return passed, {
                        "actual": actual_output,
                        "expected": test.expected_output,
                        "execution_time": execution_time,
                        "confidence": test.confidence
                    }
                
                except subprocess.TimeoutExpired:
                    process.kill()
                    return False, {
                        "timeout": True,
                        "error_code": -3,
                        "error": "Time limit exceeded"
                    }
            
            finally:
                os.unlink(temp_file)
        
        except Exception as e:
            return False, {
                "error": repr(e),
                "error_code": -4
            }
    
    def _compare_outputs(self, actual: Any, expected: Any, float_tol: float = 1e-6) -> bool:
        """
        Compare actual and expected outputs with normalization.
        Handles JSON, floats with tolerance, and normalized string comparisons.
        """
        # Try direct comparison first
        if actual == expected:
            return True
        
        # Convert to strings and normalize
        actual_str = str(actual).strip().rstrip('\n\r ')
        expected_str = str(expected).strip().rstrip('\n\r ')
        
        # Try JSON comparison
        try:
            actual_json = json.loads(actual_str)
            expected_json = json.loads(expected_str)
            return actual_json == expected_json
        except (json.JSONDecodeError, ValueError):
            pass
        
        # Try float comparison with tolerance
        try:
            actual_float = float(actual_str)
            expected_float = float(expected_str)
            return abs(actual_float - expected_float) <= float_tol
        except (ValueError, TypeError):
            pass
        
        # Normalize whitespace and simple list formatting
        def normalize_string(s: str) -> str:
            # Remove trailing newlines and spaces
            s = s.rstrip('\n\r ')
            # Collapse multiple spaces/newlines to single space
            import re
            s = re.sub(r'\s+', ' ', s)
            # Normalize simple list formats (remove spaces after commas)
            s = s.replace(', ', ',')
            return s
        
        return normalize_string(actual_str) == normalize_string(expected_str)


def compute_weighted_reward(
    test_results: List[bool],
    test_confidences: List[float],
    old_results: List[bool] = None
) -> float:
    """
    Compute reward from test results with optional confidence weighting.
    
    Args:
        test_results: List of pass/fail results
        test_confidences: List of confidence scores for each test
        old_results: Previous test results for computing delta
        
    Returns:
        Reward value (typically -0.5 to 1.0)
    """
    if not test_results:
        return 0.0
    
    # If all tests pass
    if all(test_results):
        return 1.0
    
    # Compute weighted pass rate
    weighted_passes = sum(p * c for p, c in zip(test_results, test_confidences))
    weighted_total = sum(test_confidences)
    
    if weighted_total == 0:
        return 0.0
    
    current_score = weighted_passes / weighted_total
    
    # If we have old results, compute improvement
    if old_results is not None and len(old_results) == len(test_results):
        old_weighted_passes = sum(p * c for p, c in zip(old_results, test_confidences))
        old_score = old_weighted_passes / weighted_total
        
        improvement = current_score - old_score
        
        if improvement > 0:
            return improvement  # Reward for improvement
        else:
            return -0.5  # Penalty for no improvement or regression
    else:
        # No baseline to compare, just return score
        return current_score - 0.5  # Center around 0

