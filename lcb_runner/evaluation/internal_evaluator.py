"""
Internal Evaluation Module for RL Self-Repair

This module evaluates code against synthetic tests generated by the test generator.
It provides the reward signal for the RL loop without using ground-truth tests.
"""

import json
import logging
import subprocess
import tempfile
from typing import List, Dict, Any, Tuple
import signal
import os

from lcb_runner.evaluation.synthetic_test_generator import TestCase

# Import string used by ground-truth evaluator - necessary for type hints and common imports
IMPORT_STRING = """from string import *
from re import *
from datetime import *
from collections import *
from heapq import *
from bisect import *
from copy import *
from math import *
from random import *
from statistics import *
from itertools import *
from functools import *
from operator import *
from io import *
from sys import *
from json import *
from builtins import *
from typing import *
import string
import re
import datetime
import collections
import heapq
import bisect
import copy
import math
import random
import statistics
import itertools
import functools
import operator
import io
import sys
import json
sys.setrecursionlimit(50000)
"""


class InternalEvaluator:
    """
    Evaluates code against synthetic test cases.
    Provides pass/fail results and metadata without using dataset tests.
    """
    
    def __init__(self, timeout: float = 6.0):
        self.timeout = timeout
    
    def evaluate_on_synthetic_tests(
        self,
        code: str,
        tests: List[TestCase],
        code_type: str = "stdin"
    ) -> Tuple[List[bool], Dict[str, Any]]:
        """
        Evaluate code against synthetic test cases.
        
        Args:
            code: Python code to evaluate
            tests: List of TestCase objects
            code_type: "stdin" or "call_based"
            
        Returns:
            Tuple of (pass_results, metadata)
            - pass_results: List of booleans indicating pass/fail for each test
            - metadata: Dict with test execution details
        """
        if not tests:
            return [], {"error": "No tests provided"}
        
        results = []
        metadata = {
            "test_results": [],
            "errors": [],
            "timeouts": 0,
            "execution_times": []
        }
        
        for idx, test_case in enumerate(tests):
            try:
                if code_type == "call_based":
                    passed, test_metadata = self._evaluate_call_based(code, test_case)
                else:  # stdin
                    passed, test_metadata = self._evaluate_stdin(code, test_case)
                
                results.append(passed)
                metadata["test_results"].append({
                    "index": idx,
                    "category": test_case.category,
                    "confidence": test_case.confidence,
                    "passed": passed,
                    "metadata": test_metadata
                })
                
                # Debug logging
                if not passed:
                    print(f"Test {idx} FAILED - Input: {repr(str(test_case.input_val)[:50])}, Expected: {repr(test_case.expected_output)}, Actual: {repr(test_metadata.get('actual', 'N/A'))}")
                    if 'error' in test_metadata:
                        print(f"  Error: {test_metadata['error']}")
                else:
                    print(f"Test {idx} PASSED - Category: {test_case.category}")
                
                if test_metadata.get("timeout"):
                    metadata["timeouts"] += 1
                if "execution_time" in test_metadata:
                    metadata["execution_times"].append(test_metadata["execution_time"])
                if "error" in test_metadata:
                    metadata["errors"].append({
                        "test": idx,
                        "error": test_metadata["error"]
                    })
            
            except Exception as e:
                logging.warning(f"Test {idx} evaluation failed: {e}")
                results.append(False)
                metadata["errors"].append({
                    "test": idx,
                    "error": repr(e)
                })
        
        return results, metadata
    
    def _evaluate_stdin(self, code: str, test: TestCase) -> Tuple[bool, Dict[str, Any]]:
        """Evaluate stdin-based code against a test case."""
        import time
        import ast
        
        try:
            # Check if code is empty or invalid
            if not code or code.strip() == "":
                return False, {
                    "error": "Code is empty or invalid",
                    "error_code": -5
                }
            
            # CRITICAL: Validate code syntax before execution (prevents truncation issues)
            try:
                ast.parse(code)
            except SyntaxError as e:
                return False, {
                    "error": f"Code has syntax error (possibly truncated): {str(e)[:100]}",
                    "error_code": -1,
                    "syntax_error": str(e)
                }
            
            # Create a temporary Python file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_file = f.name
            
            try:
                # Prepare input - ensure it's properly formatted
                if isinstance(test.input_val, str):
                    input_data = test.input_val
                elif isinstance(test.input_val, dict):
                    # If it's a dict, try to extract the actual input
                    if "input" in test.input_val:
                        input_data = test.input_val["input"]
                    else:
                        # Fall back to converting the whole dict to string representation
                        input_data = str(test.input_val)
                else:
                    # For other types (list, etc.), convert to string
                    input_data = str(test.input_val)
                
                # Ensure input ends with newline if it doesn't already
                if input_data and not input_data.endswith('\n'):
                    input_data += '\n'
                
                # Run with timeout
                start_time = time.time()
                process = subprocess.Popen(
                    ['python', temp_file],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                try:
                    stdout, stderr = process.communicate(
                        input=input_data, 
                        timeout=self.timeout
                    )
                    execution_time = time.time() - start_time
                    
                    if process.returncode != 0:
                        # Capture stderr for debugging
                        error_msg = stderr.strip() if stderr else "Unknown error"
                        return False, {
                            "error": error_msg,  # Don't truncate - we need full error for debugging
                            "error_code": -4,
                            "execution_time": execution_time,
                            "input_used": input_data,
                            "stderr": stderr
                        }
                    
                    # Normalize stdout: strip trailing spaces/newlines but preserve structure
                    actual_output = stdout.rstrip('\n\r ').strip()
                    expected_output = str(test.expected_output).strip()
                    
                    # Handle escaped newlines in expected output (common in test generation)
                    # If expected has literal \\n, convert to real newlines for comparison
                    if '\\n' in expected_output and '\n' not in expected_output:
                        expected_output = expected_output.replace('\\n', '\n')
                    
                    # Compare outputs with normalization
                    passed = self._compare_outputs(actual_output, expected_output)
                    
                    return passed, {
                        "actual": actual_output,
                        "expected": expected_output,
                        "execution_time": execution_time,
                        "confidence": test.confidence,
                        "input_used": input_data
                    }
                
                except subprocess.TimeoutExpired:
                    process.kill()
                    return False, {
                        "timeout": True,
                        "error_code": -3,
                        "error": "Time limit exceeded",
                        "input_used": input_data
                    }
            
            finally:
                os.unlink(temp_file)
        
        except Exception as e:
            return False, {
                "error": repr(e),
                "error_code": -4
            }
    
    def _evaluate_call_based(self, code: str, test: TestCase) -> Tuple[bool, Dict[str, Any]]:
        """Evaluate call-based code against a test case."""
        import time
        import re
        import ast
        
        try:
            # Check if code is empty or invalid
            if not code or code.strip() == "":
                return False, {
                    "error": "Code is empty or invalid",
                    "error_code": -5
                }
            
            # Validate code syntax before execution (prevents truncation issues)
            try:
                ast.parse(code)
            except SyntaxError as e:
                return False, {
                    "error": f"Code has syntax error (possibly truncated): {str(e)[:100]}",
                    "error_code": -1,
                    "syntax_error": str(e)
                }
            
            # Extract function/method name and check if it's inside a class
            func_name = None
            class_name = None
            
            lines = code.split('\n')
            for i, line in enumerate(lines):
                # Check for class definition
                class_match = re.match(r'\s*class\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)
                if class_match:
                    class_name = class_match.group(1)
                
                # Check for function/method definition
                func_match = re.match(r'\s*def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                if func_match:
                    func_name = func_match.group(1)
                    # If we found a method inside a class, we're done
                    if class_name:
                        break
            
            if not func_name:
                return False, {
                    "error": "Could not find function definition in code",
                    "error_code": -5
                }
            
            # Create test wrapper
            # Call-based inputs are always parsed as JSON strings (see grade_call_based in testing_util.py)
            # We need to handle the input correctly based on whether it's already a list/dict or a JSON string
            if isinstance(test.input_val, dict) or isinstance(test.input_val, list):
                # Already a structured object, convert to JSON and then parse
                input_json_str = json.dumps(test.input_val)
                
                # If function is inside a class, instantiate it first
                if class_name:
                    test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
input_data = json.loads({repr(input_json_str)})
obj = {class_name}()
if isinstance(input_data, list):
    result = obj.{func_name}(*input_data)
elif isinstance(input_data, dict):
    result = obj.{func_name}(**input_data)
else:
    result = obj.{func_name}(input_data)
print(json.dumps(result))
"""
                else:
                    test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
input_data = json.loads({repr(input_json_str)})
if isinstance(input_data, list):
    result = {func_name}(*input_data)
elif isinstance(input_data, dict):
    result = {func_name}(**input_data)
else:
    result = {func_name}(input_data)
print(json.dumps(result))
"""
            elif isinstance(test.input_val, str):
                # Could be a JSON string or raw string
                try:
                    # Try to parse as JSON first
                    parsed = json.loads(test.input_val)
                    if isinstance(parsed, list):
                        if class_name:
                            test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
input_data = json.loads({repr(test.input_val)})
obj = {class_name}()
result = obj.{func_name}(*input_data)
print(json.dumps(result))
"""
                        else:
                            test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
input_data = json.loads({repr(test.input_val)})
result = {func_name}(*input_data)
print(json.dumps(result))
"""
                    else:
                        if class_name:
                            test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
input_data = json.loads({repr(test.input_val)})
obj = {class_name}()
if isinstance(input_data, list):
    result = obj.{func_name}(*input_data)
elif isinstance(input_data, dict):
    result = obj.{func_name}(**input_data)
else:
    result = obj.{func_name}(input_data)
print(json.dumps(result))
"""
                        else:
                            test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
input_data = json.loads({repr(test.input_val)})
if isinstance(input_data, list):
    result = {func_name}(*input_data)
elif isinstance(input_data, dict):
    result = {func_name}(**input_data)
else:
    result = {func_name}(input_data)
print(json.dumps(result))
"""
                except json.JSONDecodeError:
                    # Not valid JSON, reject this test input as malformed
                    return False, {
                        "error": "Input is not valid JSON for call_based function",
                        "error_code": -4
                    }
            else:
                # Other types (int, float, etc.) - treat as single argument
                if class_name:
                    test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
obj = {class_name}()
result = obj.{func_name}({repr(test.input_val)})
print(json.dumps(result))
"""
                else:
                    test_wrapper = f"""
{IMPORT_STRING}

{code}

# Test execution
result = {func_name}({repr(test.input_val)})
print(json.dumps(result))
"""
            
            # Validate test_wrapper before writing (optional sanity check)
            try:
                ast.parse(test_wrapper)
            except SyntaxError as e:
                return False, {
                    "error": f"Generated test wrapper has syntax error: {str(e)[:100]}",
                    "error_code": -1,
                    "syntax_error": str(e)
                }
            
            # Create temporary file and execute
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(test_wrapper)
                temp_file = f.name
            
            try:
                start_time = time.time()
                process = subprocess.Popen(
                    ['python', temp_file],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                try:
                    stdout, stderr = process.communicate(timeout=self.timeout)
                    execution_time = time.time() - start_time
                    
                    if process.returncode != 0:
                        # Capture stderr for debugging
                        error_msg = stderr.strip() if stderr else "Unknown error"
                        return False, {
                            "error": error_msg,  # Don't truncate - we need full error for debugging
                            "error_code": -4,
                            "execution_time": execution_time
                        }
                    
                    # Parse and normalize output
                    try:
                        # Try JSON parsing first
                        actual_output = json.loads(stdout.strip())
                    except json.JSONDecodeError:
                        # Fallback to string with normalization
                        actual_output = stdout.rstrip('\n\r ').strip()
                    
                    passed = self._compare_outputs(actual_output, test.expected_output)
                    
                    return passed, {
                        "actual": actual_output,
                        "expected": test.expected_output,
                        "execution_time": execution_time,
                        "confidence": test.confidence
                    }
                
                except subprocess.TimeoutExpired:
                    process.kill()
                    return False, {
                        "timeout": True,
                        "error_code": -3,
                        "error": "Time limit exceeded"
                    }
            
            finally:
                os.unlink(temp_file)
        
        except Exception as e:
            return False, {
                "error": repr(e),
                "error_code": -4
            }
    
    def _compare_outputs(self, actual: Any, expected: Any, float_tol: float = 1e-6) -> bool:
        """
        Compare actual and expected outputs with normalization.
        Handles JSON, floats with tolerance, and normalized string comparisons.
        """
        # Try direct comparison first
        if actual == expected:
            return True
        
        # Convert to strings and normalize
        actual_str = str(actual).strip().rstrip('\n\r ')
        expected_str = str(expected).strip().rstrip('\n\r ')
        
        if actual_str == expected_str:
            return True
        
        # Handle boolean comparisons (Python True/False vs JSON true/false vs string "true"/"false")
        if actual_str.lower() in ('true', 'false') and expected_str.lower() in ('true', 'false'):
            return actual_str.lower() == expected_str.lower()
        
        # Try JSON comparison - handle cases where one is JSON-encoded and the other isn't
        try:
            # Try to parse both as JSON
            actual_json = json.loads(actual_str) if isinstance(actual, str) else actual
            expected_json = json.loads(expected_str) if isinstance(expected, str) else expected
            
            # If parsing succeeded, compare the parsed values
            if actual_json == expected_json:
                return True
        except (json.JSONDecodeError, ValueError, TypeError):
            # If one fails to parse, try parsing just the expected (might be JSON-encoded)
            try:
                expected_json = json.loads(expected_str)
                # Compare actual with the decoded expected
                if actual == expected_json or actual_str == str(expected_json):
                    return True
            except (json.JSONDecodeError, ValueError, TypeError):
                pass
        
        # Try float comparison with tolerance
        try:
            actual_float = float(actual_str)
            expected_float = float(expected_str)
            return abs(actual_float - expected_float) <= float_tol
        except (ValueError, TypeError):
            pass
        
        # For multi-line outputs, compare line by line
        actual_lines = actual_str.split('\n')
        expected_lines = expected_str.split('\n')
        
        if len(actual_lines) == len(expected_lines):
            all_match = True
            for a_line, e_line in zip(actual_lines, expected_lines):
                a_line = a_line.strip()
                e_line = e_line.strip()
                
                # Try numeric comparison per line
                try:
                    if abs(float(a_line) - float(e_line)) > float_tol:
                        all_match = False
                        break
                except (ValueError, TypeError):
                    # String comparison with normalization
                    if self._normalize_string(a_line) != self._normalize_string(e_line):
                        all_match = False
                        break
            
            if all_match:
                return True
        
        # Fallback to normalized string comparison
        return self._normalize_string(actual_str) == self._normalize_string(expected_str)
    
    def _normalize_string(self, s: str) -> str:
        """Normalize string for comparison."""
        # Remove trailing newlines and spaces
        s = s.rstrip('\n\r ')
        # Collapse multiple spaces/newlines to single space
        import re
        s = re.sub(r'\s+', ' ', s)
        # Normalize simple list formats (remove spaces after commas)
        s = s.replace(', ', ',')
        return s


def compute_weighted_reward(
    test_results: List[bool],
    test_confidences: List[float],
    old_results: List[bool] = None
) -> float:
    """
    Compute reward from test results with optional confidence weighting.
    Enhanced to give better signal for partial progress.
    
    Args:
        test_results: List of pass/fail results
        test_confidences: List of confidence scores for each test
        old_results: Previous test results for computing delta
        
    Returns:
        Reward value (typically -0.3 to 1.0)
    """
    if not test_results:
        return 0.0
    
    # If all tests pass - BIG reward!
    if all(test_results):
        return 1.0
    
    # Compute weighted pass rate
    weighted_passes = sum(p * c for p, c in zip(test_results, test_confidences))
    weighted_total = sum(test_confidences)
    
    if weighted_total == 0:
        return 0.0
    
    current_score = weighted_passes / weighted_total
    
    # If we have old results, compute improvement
    if old_results is not None and len(old_results) == len(test_results):
        old_weighted_passes = sum(p * c for p, c in zip(old_results, test_confidences))
        old_score = old_weighted_passes / weighted_total
        
        improvement = current_score - old_score
        
        if improvement > 0.1:  # Significant improvement
            return min(0.8, improvement * 2)  # Amplify reward (up to 0.8)
        elif improvement > 0:  # Small improvement
            return improvement * 0.5  # Moderate reward
        elif improvement > -0.1:  # Same or slight regression
            return -0.1  # Small penalty (reduced from -0.5)
        else:  # Significant regression
            return -0.3  # Moderate penalty (reduced from -0.5)
    else:
        # No baseline to compare, reward based on absolute performance
        # Scale: 0→-0.3, 0.5→0.1, 1.0→0.7
        return (current_score - 0.3)  # More generous than before (-0.5)

